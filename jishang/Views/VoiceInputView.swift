//
//  VoiceInputView.swift
//  jishang
//
//  Created by Gnl on 2025/9/9.
//

import SwiftUI
import Speech
import AVFoundation

struct VoiceInputView: View {
    @Binding var isPresented: Bool
    let transactionType: TransactionType
    let onVoiceResult: (String) -> Void
    
    @StateObject private var voiceRecognizer = VoiceRecognizer()
    @State private var isRecording = false
    @State private var recordingTime: TimeInterval = 0
    @State private var timer: Timer?
    
    var body: some View {
        NavigationView {
            VStack(spacing: 30) {
                // æ ‡é¢˜
                Text(transactionType == .income ? "è¯­éŸ³è®°å½•æ”¶å…¥" : "è¯­éŸ³è®°å½•æ”¯å‡º")
                    .font(.title2)
                    .fontWeight(.semibold)
                
                // å½•éŸ³çŠ¶æ€æŒ‡ç¤ºå™¨
                VStack(spacing: 16) {
                    ZStack {
                        Circle()
                            .fill(isRecording ? Color.red.opacity(0.2) : Color.gray.opacity(0.2))
                            .frame(width: 120, height: 120)
                            .scaleEffect(isRecording ? 1.2 : 1.0)
                            .animation(.easeInOut(duration: 0.6).repeatForever(autoreverses: true), value: isRecording)
                        
                        Image(systemName: isRecording ? "mic.fill" : "mic")
                            .font(.system(size: 40))
                            .foregroundColor(isRecording ? .red : .gray)
                    }
                    
                    if isRecording {
                        Text(String(format: "%.1fç§’", recordingTime))
                            .font(.title3)
                            .foregroundColor(.secondary)
                    }
                }
                
                // è¯†åˆ«ç»“æœ
                if !voiceRecognizer.recognizedText.isEmpty {
                    VStack(alignment: .leading, spacing: 12) {
                        Text("è¯†åˆ«ç»“æœ:")
                            .font(.headline)
                            .foregroundColor(.secondary)
                        
                        Text(voiceRecognizer.recognizedText)
                            .font(.body)
                            .padding()
                            .background(Color(.systemGray6))
                            .cornerRadius(12)
                        
                        // æ·»åŠ è§£æç»“æœæ˜¾ç¤ºç”¨äºè°ƒè¯•
                        if let parsedResult = parseVoiceTextForDebug(voiceRecognizer.recognizedText) {
                            VStack(alignment: .leading, spacing: 8) {
                                Text("è§£æç»“æœ (è°ƒè¯•):")
                                    .font(.subheadline)
                                    .foregroundColor(.orange)
                                Text("ç»“æœ: \(voiceRecognizer.recognizedText)")
                                VStack(alignment: .leading, spacing: 4) {
                                    Text("é‡‘é¢: Â¥\(String(format: "%.2f", parsedResult.amount))")
                                    Text("ç±»å‹: \(parsedResult.type == .income ? "æ”¶å…¥" : "æ”¯å‡º")")
                                    Text("ç±»åˆ«: \(parsedResult.category ?? "æ— ")")
                                    Text("æè¿°: \(parsedResult.description)")
                                }
                                .font(.caption)
                                .foregroundColor(.secondary)
                            }
                            .padding()
                            .background(Color.orange.opacity(0.1))
                            .cornerRadius(8)
                        }
                    }
                    .padding(.horizontal)
                }
                
                // æç¤ºæ–‡æœ¬
                if !isRecording && voiceRecognizer.recognizedText.isEmpty {
                    Text("é•¿æŒ‰å¼€å§‹å½•éŸ³ï¼Œè¯´å‡ºæ‚¨çš„è®°è´¦ä¿¡æ¯\nä¾‹å¦‚: \"åˆé¥­èŠ±äº†30å…ƒ\" æˆ– \"å·¥èµ„æ”¶å…¥5000å…ƒ\"")
                        .font(.body)
                        .foregroundColor(.secondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal)
                }
                
                Spacer()
                
                // å½•éŸ³æŒ‰é’®
                VStack(spacing: 16) {
                    Button(action: {}) {
                        Circle()
                            .fill(isRecording ? Color.red : transactionType == .income ? Color.blue.opacity(0.7) : Color.red.opacity(0.7))
                            .frame(width: 80, height: 80)
                            .overlay(
                                RoundedRectangle(cornerRadius: isRecording ? 12 : 40)
                                    .fill(Color.white)
                                    .frame(width: isRecording ? 24 : 30, height: isRecording ? 24 : 30)
                                    .animation(.easeInOut(duration: 0.2), value: isRecording)
                            )
                    }
                    .scaleEffect(isRecording ? 1.1 : 1.0)
                    .animation(.easeInOut(duration: 0.2), value: isRecording)
                    .onLongPressGesture(minimumDuration: 0.1, maximumDistance: .infinity, perform: {
                        stopRecording()
                    }, onPressingChanged: { pressing in
                        if pressing {
                            startRecording()
                        } else {
                            stopRecording()
                        }
                    })
                    
                    Text(isRecording ? "æ¾å¼€ç»“æŸå½•éŸ³" : "é•¿æŒ‰å¼€å§‹å½•éŸ³")
                        .font(.caption)
                        .foregroundColor(.secondary)
                }
                
                // æ“ä½œæŒ‰é’®
                HStack(spacing: 20) {
                    Button("å–æ¶ˆ") {
                        isPresented = false
                    }
                    .foregroundColor(.secondary)
                    
                    Button("ç¡®è®¤") {
                        onVoiceResult(voiceRecognizer.recognizedText)
                        isPresented = false
                    }
                    .disabled(voiceRecognizer.recognizedText.isEmpty)
                    .foregroundColor(voiceRecognizer.recognizedText.isEmpty ? .secondary : .blue)
                    .fontWeight(.semibold)
                }
                .padding(.bottom)
            }
            .padding()
            .navigationBarTitleDisplayMode(.inline)
            .navigationBarHidden(true)
        }
        .onAppear {
            voiceRecognizer.requestPermission()
        }
        .onDisappear {
            stopRecording()
        }
    }
    
    // è°ƒè¯•ç”¨çš„è§£æå‡½æ•°
    private func parseVoiceTextForDebug(_ text: String) -> ParsedTransaction? {
        let parser = VoiceTransactionParser()
        return parser.parseVoiceText(text, expectedType: transactionType)
    }
    
    private func startRecording() {
        guard !isRecording else { return }
        
        isRecording = true
        recordingTime = 0
        
        // éœ‡åŠ¨åé¦ˆ
        let impactFeedback = UIImpactFeedbackGenerator(style: .medium)
        impactFeedback.impactOccurred()
        
        // å¼€å§‹å½•éŸ³
        voiceRecognizer.startRecording()
        
        // å¼€å§‹è®¡æ—¶
        timer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { _ in
            recordingTime += 0.1
        }
    }
    
    private func stopRecording() {
        guard isRecording else { return }
        
        isRecording = false
        timer?.invalidate()
        timer = nil
        
        // éœ‡åŠ¨åé¦ˆ
        let impactFeedback = UIImpactFeedbackGenerator(style: .light)
        impactFeedback.impactOccurred()
        
        // åœæ­¢å½•éŸ³
        voiceRecognizer.stopRecording()
        
        // è°ƒè¯•æ‰“å°
        print("ğŸ¤ å½•éŸ³åœæ­¢ï¼Œè¯†åˆ«åˆ°çš„æ–‡æœ¬: \(voiceRecognizer.recognizedText)")
        if let parsed = parseVoiceTextForDebug(voiceRecognizer.recognizedText) {
            print("ğŸ§  è§£æç»“æœ: é‡‘é¢=Â¥\(parsed.amount), ç±»å‹=\(parsed.type), ç±»åˆ«=\(parsed.category ?? "æ— "), æè¿°=\(parsed.description)")
        } else {
            print("âŒ è§£æå¤±è´¥")
        }
    }
}

class VoiceRecognizer: NSObject, ObservableObject {
    @Published var recognizedText = ""
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    
    func requestPermission() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                // å¤„ç†æƒé™çŠ¶æ€
            }
        }
        
        AVAudioSession.sharedInstance().requestRecordPermission { _ in
            // å¤„ç†å½•éŸ³æƒé™
        }
    }
    
    func startRecording() {
        // é‡ç½®ä¹‹å‰çš„è¯†åˆ«ç»“æœ
        recognizedText = ""
        
        // é…ç½®éŸ³é¢‘ä¼šè¯
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error)")
            return
        }
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        recognitionRequest.shouldReportPartialResults = true
        
        // è·å–éŸ³é¢‘è¾“å…¥èŠ‚ç‚¹
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // å®‰è£…éŸ³é¢‘tap
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }
        
        // å‡†å¤‡å’Œå¯åŠ¨éŸ³é¢‘å¼•æ“
        audioEngine.prepare()
        do {
            try audioEngine.start()
        } catch {
            print("éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥: \(error)")
            return
        }
        
        // å¼€å§‹è¯†åˆ«
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            DispatchQueue.main.async {
                if let result = result {
                    self.recognizedText = result.bestTranscription.formattedString
                    print("ğŸ”„ å®æ—¶è¯†åˆ«: \(self.recognizedText)")
                }
                
                if let error = error {
                    print("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error)")
                }
                
                if error != nil || result?.isFinal == true {
                    if result?.isFinal == true {
                        print("âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ: \(self.recognizedText)")
                    }
                    self.stopRecording()
                }
            }
        }
    }
    
    func stopRecording() {
        recognitionRequest?.endAudio()
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest = nil
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // æ¢å¤éŸ³é¢‘ä¼šè¯
        do {
            try AVAudioSession.sharedInstance().setActive(false)
        } catch {
            print("éŸ³é¢‘ä¼šè¯æ¢å¤å¤±è´¥: \(error)")
        }
    }
}

#Preview {
    VoiceInputView(
        isPresented: .constant(true),
        transactionType: .expense,
        onVoiceResult: { result in
            print("Voice result: \(result)")
        }
    )
}
